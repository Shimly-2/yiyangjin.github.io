---
title:          RoboLLM: A Unified Multimodal Vision-Language-Action Framework for Perception, Understanding, and Generation  
date:           2025-07-15 00:01:00 +0800
selected:       true
pub:            "RA-L"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
pub_last:       ' <span class="badge badge-pill badge-custom badge-secondary">Journal</span>'
pub_date:       "2025"

abstract: >-
  Recent progress in multimodal learning has demonstrated the potential for unified frameworks that integrate diverse sensory modalities, yet existing approaches struggle with the heterogeneous nature of robotic perception and the challenge of bridging understanding with action generation. To address these limitations, we propose RoboLLM, a unified multimodal vision-language-action framework that seamlessly integrates perception, understanding, and generation capabilities for robotic applications. RoboLLM introduces a novel Multimodal Vector Quantized Variational Autoencoder (MMVQ-VAE) that unifies discrete encoding of heterogeneous perceptual information, including vision, touch, proprioception, and language, while maintaining consistency with pre-trained models through native tokenization. We design a Dense Prompt-based annotation system with strategic token combinations that enables structured arrangement of multimodal information for effective cross-modal reasoning. Our hybrid pre-training paradigm combines MagViT-based masked prediction for visual and tactile modalities with autoregressive modeling for text and proprioception, facilitating collaborative multimodal learning. RoboLLM demonstrates versatile capabilities across downstream tasks, including generative modeling of tactile signals and visual imagery, robot action prediction, and comprehensive visual understanding with multimodal question-answering. 



cover:          assets/images/covers/robollm.png
authors:
  - Yanmin Zhou*  
  - Yiyang Jin  
  - Rong Jiang  
  - Xin Li  
  - Hongrui Sang  
  - Shuo Jiang  
  - Zhipeng Wangâ€   
  - Bin He
# links:
#   Dataset: https://huggingface.co/datasets/xsdfasfgsa/VLaPT
  
---


